# src/services/vocabulary_generator.py
"""
Servi√ßo de gera√ß√£o de vocabul√°rio com RAG e an√°lise de imagens.
Implementa√ß√£o completa do PROMPT 6 do IVO V2 Guide.
CORRIGIDO: IA contextual para an√°lises complexas, constantes t√©cnicas mantidas.
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Any, Optional
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from pydantic import BaseModel, ValidationError

from src.core.unit_models import (
    VocabularyItem, VocabularySection, VocabularyGenerationRequest,
    VocabularyGenerationResponse
)
from src.core.enums import CEFRLevel, LanguageVariant, UnitType
from config.models import get_openai_config, load_model_configs

logger = logging.getLogger(__name__)


# =============================================================================
# CONSTANTES T√âCNICAS (MANTIDAS - S√ÉO PADR√ïES ESTABELECIDOS)
# =============================================================================

IPA_VARIANT_MAPPING = {
    "american_english": "general_american",
    "british_english": "received_pronunciation",
    "australian_english": "australian_english",
    "canadian_english": "canadian_english",
    "indian_english": "indian_english"
}

VOWEL_SOUNDS = "aeiouy"

STRESS_PATTERNS = {
    "primary_first": "Àà",
    "secondary_first": "Àå",
    "unstressed": ""
}


class VocabularyGeneratorService:
    """Servi√ßo principal para gera√ß√£o de vocabul√°rio contextual."""
    
    def __init__(self):
        """Inicializar servi√ßo com configura√ß√µes."""
        self.openai_config = get_openai_config()
        self.model_configs = load_model_configs()
        
        # Configurar LangChain LLM
        self.llm = ChatOpenAI(
            model=self.openai_config.openai_model,
            temperature=0.6,  # Criatividade moderada para vocabul√°rio
            max_tokens=2048,
            api_key=self.openai_config.openai_api_key
        )
        
        logger.info("‚úÖ VocabularyGeneratorService inicializado com IA contextual")
    
    async def generate_vocabulary_for_unit(
        self, 
        generation_params: Dict[str, Any]
    ) -> VocabularySection:
        """
        Gerar vocabul√°rio para uma unidade usando RAG e an√°lise de imagens.
        
        Args:
            generation_params: Par√¢metros com contexto da unidade, RAG e imagens
            
        Returns:
            VocabularySection completa com itens validados
        """
        try:
            start_time = time.time()
            
            # Extrair par√¢metros
            unit_data = generation_params.get("unit_data", {})
            hierarchy_context = generation_params.get("hierarchy_context", {})
            rag_context = generation_params.get("rag_context", {})
            images_analysis = generation_params.get("images_analysis", {})
            target_count = generation_params.get("target_vocabulary_count", 25)
            
            logger.info(f"üî§ Gerando {target_count} palavras de vocabul√°rio para unidade")
            
            # 1. Construir contexto enriquecido
            enriched_context = await self._build_enriched_context(
                unit_data, hierarchy_context, rag_context, images_analysis
            )
            
            # 2. AN√ÅLISE VIA IA: Guidelines CEFR contextuais
            cefr_guidelines = await self._analyze_cefr_guidelines_ai(
                cefr_level=unit_data.get("cefr_level", "A2"),
                unit_context=unit_data.get("context", ""),
                unit_type=unit_data.get("unit_type", "lexical_unit"),
                hierarchy_context=hierarchy_context
            )
            
            # 3. Gerar prompt contextualizado
            vocabulary_prompt = await self._build_vocabulary_prompt(
                enriched_context, target_count, cefr_guidelines
            )
            
            # 4. Gerar vocabul√°rio via LLM
            raw_vocabulary = await self._generate_vocabulary_llm(vocabulary_prompt)
            
            # 5. Processar e validar vocabul√°rio
            validated_items = await self._process_and_validate_vocabulary(
                raw_vocabulary, enriched_context
            )
            
            # 6. Aplicar RAG para evitar repeti√ß√µes
            filtered_items = await self._apply_rag_filtering(
                validated_items, rag_context
            )
            
            # 7. Enriquecer com fonemas IPA
            enriched_items = await self._enrich_with_phonemes_ai(
                filtered_items, unit_data.get("language_variant", "american_english"),
                unit_data.get("context", "")
            )
            
            # 8. AN√ÅLISE VIA IA: M√©tricas de qualidade
            quality_metrics = await self._calculate_quality_metrics_ai(
                enriched_items, enriched_context, rag_context
            )
            
            # 9. AN√ÅLISE VIA IA: Complexidade fon√©tica
            phonetic_complexity = await self._analyze_phonetic_complexity_ai(
                enriched_items, unit_data.get("cefr_level", "A2")
            )
            
            # 10. Construir resposta final
            vocabulary_section = VocabularySection(
                items=enriched_items[:target_count],  # Limitar ao n√∫mero desejado
                total_count=len(enriched_items[:target_count]),
                context_relevance=quality_metrics.get("context_relevance", 0.8),
                new_words_count=quality_metrics.get("new_words_count", len(enriched_items)),
                reinforcement_words_count=quality_metrics.get("reinforcement_count", 0),
                rag_context_used=rag_context,
                progression_level=rag_context.get("progression_level", "intermediate"),
                phoneme_coverage=quality_metrics.get("phoneme_coverage", {}),
                pronunciation_variants=[unit_data.get("language_variant", "american_english")],
                phonetic_complexity=phonetic_complexity,
                generated_at=datetime.now()
            )
            
            generation_time = time.time() - start_time
            
            logger.info(
                f"‚úÖ Vocabul√°rio gerado: {len(enriched_items)} palavras em {generation_time:.2f}s"
            )
            
            return vocabulary_section
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de vocabul√°rio: {str(e)}")
            raise
    
    async def _build_enriched_context(
        self,
        unit_data: Dict[str, Any],
        hierarchy_context: Dict[str, Any],
        rag_context: Dict[str, Any],
        images_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Construir contexto enriquecido para gera√ß√£o."""
        
        # Extrair vocabul√°rio das imagens se dispon√≠vel
        image_vocabulary = []
        if images_analysis.get("success") and images_analysis.get("consolidated_vocabulary"):
            vocab_data = images_analysis["consolidated_vocabulary"].get("vocabulary", [])
            image_vocabulary = [item.get("word", "") for item in vocab_data if item.get("word")]
        
        # Contexto base da unidade
        base_context = unit_data.get("context", "")
        unit_title = unit_data.get("title", "")
        
        # Contexto da hierarquia
        course_name = hierarchy_context.get("course_name", "")
        book_name = hierarchy_context.get("book_name", "")
        sequence_order = hierarchy_context.get("sequence_order", 1)
        
        # An√°lise de progress√£o
        taught_vocabulary = rag_context.get("taught_vocabulary", [])
        progression_level = rag_context.get("progression_level", "intermediate")
        
        # Temas contextuais das imagens
        image_themes = []
        if images_analysis.get("success"):
            for analysis in images_analysis.get("individual_analyses", []):
                if "structured_data" in analysis.get("analysis", {}):
                    themes = analysis["analysis"]["structured_data"].get("contextual_themes", [])
                    image_themes.extend(themes)
        
        enriched_context = {
            "unit_context": {
                "title": unit_title,
                "context": base_context,
                "cefr_level": unit_data.get("cefr_level", "A2"),
                "language_variant": unit_data.get("language_variant", "american_english"),
                "unit_type": unit_data.get("unit_type", "lexical_unit")
            },
            "hierarchy_context": {
                "course_name": course_name,
                "book_name": book_name,
                "sequence_order": sequence_order,
                "target_level": hierarchy_context.get("target_level", unit_data.get("cefr_level"))
            },
            "rag_context": {
                "taught_vocabulary": taught_vocabulary[:20],  # √öltimas 20 para contexto
                "progression_level": progression_level,
                "vocabulary_density": rag_context.get("vocabulary_density", 0),
                "words_to_avoid": taught_vocabulary,
                "reinforcement_candidates": self._select_reinforcement_words(taught_vocabulary)
            },
            "images_context": {
                "vocabulary_suggestions": image_vocabulary[:15],  # Top 15 das imagens
                "themes": list(set(image_themes))[:10],  # Top 10 temas √∫nicos
                "has_images": bool(image_vocabulary),
                "images_analyzed": len(images_analysis.get("individual_analyses", []))
            },
            "generation_preferences": {
                "target_count": 25,
                "allow_reinforcement": True,
                "focus_on_images": bool(image_vocabulary),
                "progression_appropriate": True
            }
        }
        
        return enriched_context
    
    # =============================================================================
    # AN√ÅLISES VIA IA (SUBSTITUEM DADOS HARD-CODED)
    # =============================================================================
    
    async def _analyze_cefr_guidelines_ai(
        self, 
        cefr_level: str, 
        unit_context: str, 
        unit_type: str,
        hierarchy_context: Dict[str, Any]
    ) -> str:
        """An√°lise contextual via IA para guidelines CEFR espec√≠ficas."""
        
        system_prompt = """Voc√™ √© um especialista em n√≠veis CEFR e desenvolvimento de vocabul√°rio contextual.
        
        Analise o n√≠vel CEFR fornecido considerando o contexto espec√≠fico da unidade e tipo de ensino.
        Forne√ßa guidelines espec√≠ficas e contextuais para sele√ß√£o de vocabul√°rio apropriado."""
        
        human_prompt = f"""Analise este contexto educacional espec√≠fico:
        
        N√çVEL CEFR: {cefr_level}
        CONTEXTO DA UNIDADE: {unit_context}
        TIPO DE UNIDADE: {unit_type}
        CURSO: {hierarchy_context.get('course_name', '')}
        LIVRO: {hierarchy_context.get('book_name', '')}
        SEQU√äNCIA: Unidade {hierarchy_context.get('sequence_order', 1)}
        
        Forne√ßa guidelines espec√≠ficas para sele√ß√£o de vocabul√°rio considerando:
        - Complexidade apropriada para o n√≠vel {cefr_level}
        - Relev√¢ncia espec√≠fica ao contexto "{unit_context}"
        - Progress√£o pedag√≥gica adequada
        - Aplicabilidade comunicativa no contexto espec√≠fico
        - Adequa√ß√£o ao tipo de unidade {unit_type}
        
        Responda com guidelines diretas e espec√≠ficas para este contexto exato, n√£o gen√©ricas."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            return response.content.strip()
            
        except Exception as e:
            logger.warning(f"Erro na an√°lise CEFR via IA: {str(e)}")
            return self._minimal_cefr_fallback(cefr_level)
    
    async def _analyze_phonetic_complexity_ai(
        self, 
        vocabulary_items: List[VocabularyItem], 
        cefr_level: str
    ) -> str:
        """An√°lise contextual via IA da complexidade fon√©tica."""
        
        system_prompt = """Voc√™ √© um especialista em an√°lise fon√©tica e complexidade de pron√∫ncia para brasileiros.
        
        Analise a complexidade fon√©tica do vocabul√°rio considerando:
        - Padr√µes de s√≠labas e stress
        - Sons desafiadores para brasileiros
        - Adequa√ß√£o ao n√≠vel CEFR
        - Distribui√ß√£o de dificuldades"""
        
        # Preparar dados do vocabul√°rio
        vocab_analysis = []
        for item in vocabulary_items[:10]:  # Limitar para an√°lise
            word_info = f"{item.word}"
            if item.phoneme:
                word_info += f" [{item.phoneme}]"
            if item.syllable_count:
                word_info += f" ({item.syllable_count} s√≠labas)"
            vocab_analysis.append(word_info)
        
        human_prompt = f"""Analise a complexidade fon√©tica deste vocabul√°rio:
        
        VOCABUL√ÅRIO: {'; '.join(vocab_analysis)}
        N√çVEL CEFR: {cefr_level}
        TOTAL DE PALAVRAS: {len(vocabulary_items)}
        
        Analise:
        - N√≠vel geral de complexidade fon√©tica
        - Padr√µes de stress predominantes
        - Sons espec√≠ficos desafiadores para brasileiros
        - Distribui√ß√£o de complexidade sil√°bica
        - Adequa√ß√£o ao n√≠vel {cefr_level}
        
        Retorne classifica√ß√£o: "simple", "medium", "complex", ou "very_complex" com justificativa."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Extrair classifica√ß√£o da resposta
            content = response.content.lower()
            if "very_complex" in content:
                return "very_complex"
            elif "complex" in content:
                return "complex"
            elif "simple" in content:
                return "simple"
            else:
                return "medium"
                
        except Exception as e:
            logger.warning(f"Erro na an√°lise fon√©tica via IA: {str(e)}")
            return "medium"
    
    async def _calculate_quality_metrics_ai(
        self, 
        vocabulary_items: List[VocabularyItem], 
        enriched_context: Dict[str, Any],
        rag_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Calcular m√©tricas de qualidade usando an√°lise IA."""
        
        if not vocabulary_items:
            return {"context_relevance": 0.0, "new_words_count": 0, "reinforcement_count": 0}
        
        # Contagens b√°sicas (mantidas - s√£o contadores simples)
        new_words = [item for item in vocabulary_items if not item.is_reinforcement]
        reinforcement_words = [item for item in vocabulary_items if item.is_reinforcement]
        
        # AN√ÅLISE VIA IA: Relev√¢ncia contextual
        context_relevance = await self._analyze_context_relevance_ai(
            vocabulary_items, enriched_context
        )
        
        # Cobertura de fonemas (mantida - √© an√°lise t√©cnica)
        phonemes_used = set()
        for item in vocabulary_items:
            if item.phoneme:
                clean_phoneme = item.phoneme.strip('/[]')
                phonemes_used.update(clean_phoneme.replace(' ', ''))
        
        phoneme_coverage = {
            "total_unique_phonemes": len(phonemes_used),
            "coverage_score": min(len(phonemes_used) / 30, 1.0)
        }
        
        # Distribui√ß√£o por classe de palavra (mantida - √© contagem)
        word_classes = {}
        for item in vocabulary_items:
            word_class = item.word_class
            word_classes[word_class] = word_classes.get(word_class, 0) + 1
        
        return {
            "context_relevance": context_relevance,
            "new_words_count": len(new_words),
            "reinforcement_count": len(reinforcement_words),
            "phoneme_coverage": phoneme_coverage,
            "word_class_distribution": word_classes,
            "quality_score": (context_relevance + phoneme_coverage["coverage_score"]) / 2
        }
    
    async def _analyze_context_relevance_ai(
        self, 
        vocabulary_items: List[VocabularyItem], 
        enriched_context: Dict[str, Any]
    ) -> float:
        """An√°lise contextual via IA da relev√¢ncia do vocabul√°rio."""
        
        system_prompt = """Voc√™ √© um especialista em avalia√ß√£o de relev√¢ncia contextual de vocabul√°rio.
        
        Analise qu√£o relevante o vocabul√°rio √© para o contexto espec√≠fico da unidade."""
        
        # Preparar vocabul√°rio para an√°lise
        vocab_summary = [f"{item.word} ({item.word_class})" for item in vocabulary_items[:15]]
        unit_ctx = enriched_context.get("unit_context", {})
        
        human_prompt = f"""Avalie a relev√¢ncia contextual deste vocabul√°rio:
        
        VOCABUL√ÅRIO: {', '.join(vocab_summary)}
        CONTEXTO DA UNIDADE: {unit_ctx.get('context', '')}
        T√çTULO: {unit_ctx.get('title', '')}
        N√çVEL: {unit_ctx.get('cefr_level', 'A2')}
        TIPO: {unit_ctx.get('unit_type', 'lexical_unit')}
        
        Avalie numa escala de 0.0 a 1.0:
        - Qu√£o relevante este vocabul√°rio √© para o contexto espec√≠fico
        - Se as palavras s√£o apropriadas para o cen√°rio
        - Se contribuem para objetivos comunicativos do contexto
        
        Retorne APENAS um n√∫mero decimal entre 0.0 e 1.0 representando a relev√¢ncia."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Extrair score num√©rico
            import re
            score_match = re.search(r'0\.\d+|1\.0', response.content)
            if score_match:
                return float(score_match.group())
            else:
                return 0.7  # Fallback padr√£o
                
        except Exception as e:
            logger.warning(f"Erro na an√°lise de relev√¢ncia via IA: {str(e)}")
            return 0.7
    
    async def _enrich_with_phonemes_ai(
        self, 
        vocabulary_items: List[VocabularyItem], 
        language_variant: str,
        unit_context: str
    ) -> List[VocabularyItem]:
        """Enriquecer itens com fonemas IPA usando an√°lise IA quando necess√°rio."""
        
        items_needing_phonemes = []
        complete_items = []
        
        # Separar itens que precisam de melhoria fon√©tica
        for item in vocabulary_items:
            if not item.phoneme or item.phoneme.startswith("/placeholder_"):
                items_needing_phonemes.append(item)
            else:
                # Aplicar variante IPA (constante t√©cnica mantida)
                item.ipa_variant = self._get_ipa_variant(language_variant)
                item.stress_pattern = self._estimate_stress_pattern(item.phoneme)
                complete_items.append(item)
        
        # AN√ÅLISE VIA IA: Gerar fonemas para itens que precisam
        if items_needing_phonemes:
            improved_items = await self._improve_phonemes_ai(
                items_needing_phonemes, language_variant, unit_context
            )
            complete_items.extend(improved_items)
        
        return complete_items
    
    async def _improve_phonemes_ai(
        self, 
        vocabulary_items: List[VocabularyItem], 
        language_variant: str,
        unit_context: str
    ) -> List[VocabularyItem]:
        """Melhorar fonemas usando IA contextual."""
        
        system_prompt = f"""Voc√™ √© um especialista em fon√©tica inglesa e transcri√ß√£o IPA.
        
        Forne√ßa transcri√ß√µes IPA precisas para as palavras, considerando a variante {language_variant} e o contexto espec√≠fico."""
        
        words_to_improve = [item.word for item in vocabulary_items[:10]]  # Limitar para performance
        
        human_prompt = f"""Forne√ßa transcri√ß√µes IPA para estas palavras:
        
        PALAVRAS: {', '.join(words_to_improve)}
        VARIANTE: {language_variant}
        CONTEXTO: {unit_context}
        
        Para cada palavra, retorne no formato: palavra: /transcri√ß√£o/
        Use IPA padr√£o para {language_variant}.
        Considere o contexto para poss√≠veis varia√ß√µes de pron√∫ncia.
        
        Exemplo formato:
        hotel: /ho äÀàt…õl/
        reception: /r…™Ààs…õp É…ôn/"""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Parse da resposta
            phoneme_mapping = self._parse_phoneme_response(response.content)
            
            # Aplicar melhorias
            for item in vocabulary_items:
                if item.word in phoneme_mapping:
                    item.phoneme = phoneme_mapping[item.word]
                    item.ipa_variant = self._get_ipa_variant(language_variant)
                    item.stress_pattern = self._estimate_stress_pattern(item.phoneme)
                else:
                    # Fallback t√©cnico para palavras n√£o encontradas
                    item.phoneme = self._generate_basic_phoneme(item.word)
                    item.ipa_variant = self._get_ipa_variant(language_variant)
            
            return vocabulary_items
            
        except Exception as e:
            logger.warning(f"Erro na melhoria de fonemas via IA: {str(e)}")
            # Aplicar fallbacks t√©cnicos
            for item in vocabulary_items:
                item.phoneme = self._generate_basic_phoneme(item.word)
                item.ipa_variant = self._get_ipa_variant(language_variant)
            return vocabulary_items
    
    async def _build_vocabulary_prompt(
        self, 
        enriched_context: Dict[str, Any], 
        target_count: int,
        cefr_guidelines: str
    ) -> List[Any]:
        """Construir prompt contextualizado para gera√ß√£o de vocabul√°rio."""
        
        unit_ctx = enriched_context["unit_context"]
        hierarchy_ctx = enriched_context["hierarchy_context"]
        rag_ctx = enriched_context["rag_context"]
        images_ctx = enriched_context["images_context"]
        
        system_prompt = f"""You are an expert English vocabulary teacher creating contextualized vocabulary for {unit_ctx['cefr_level']} level students.

EDUCATIONAL CONTEXT:
- Course: {hierarchy_ctx['course_name']}
- Book: {hierarchy_ctx['book_name']}
- Unit: {unit_ctx['title']}
- Sequence: Unit {hierarchy_ctx['sequence_order']} of the book
- Context: {unit_ctx['context']}
- Level: {unit_ctx['cefr_level']}
- Language Variant: {unit_ctx['language_variant']}
- Unit Type: {unit_ctx['unit_type']}

CONTEXTUAL CEFR GUIDELINES: {cefr_guidelines}

RAG CONTEXT (Important for coherence):
- Words already taught: {', '.join(rag_ctx['taught_vocabulary'])}
- Progression level: {rag_ctx['progression_level']}
- Reinforcement candidates: {', '.join(rag_ctx['reinforcement_candidates'][:5])}

IMAGES ANALYSIS:
{"- Image vocabulary suggestions: " + ', '.join(images_ctx['vocabulary_suggestions']) if images_ctx['vocabulary_suggestions'] else "- No images analyzed"}
{"- Image themes: " + ', '.join(images_ctx['themes']) if images_ctx['themes'] else ""}

GENERATION REQUIREMENTS:
1. Generate exactly {target_count} vocabulary items
2. Avoid repeating words from "already taught" list unless for reinforcement
3. Focus on words visible/suggested in images when available
4. Ensure vocabulary follows the contextual guidelines above
5. Include 10-20% reinforcement words for review
6. Each word must include: word, IPA phoneme, Portuguese definition, contextual example
7. Prioritize practical, communicative vocabulary for this specific context
8. Use {unit_ctx['language_variant']} pronunciations

OUTPUT FORMAT: Return valid JSON array with this exact structure:
[
  {{
    "word": "example",
    "phoneme": "/…™…°Ààz√¶mp…ôl/",
    "definition": "exemplo, modelo",
    "example": "This is a good example of modern architecture.",
    "word_class": "noun",
    "frequency_level": "high",
    "context_relevance": 0.9,
    "is_reinforcement": false
  }}
]"""

        human_prompt = f"""Generate {target_count} vocabulary items for the unit "{unit_ctx['title']}" in the context: "{unit_ctx['context']}"

Level: {unit_ctx['cefr_level']}
Type: {unit_ctx['unit_type']}

{"Image suggestions to prioritize: " + ', '.join(images_ctx['vocabulary_suggestions'][:10]) if images_ctx['vocabulary_suggestions'] else "No image context available."}

Remember:
- Avoid: {', '.join(rag_ctx['words_to_avoid'][:10])}
- Consider for reinforcement: {', '.join(rag_ctx['reinforcement_candidates'][:5])}
- Follow the contextual guidelines provided
- Make examples relevant to the specific context
- Ensure {unit_ctx['language_variant']} pronunciation

Generate the JSON array now:"""

        return [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]
    
    async def _generate_vocabulary_llm(self, prompt_messages: List[Any]) -> List[Dict[str, Any]]:
        """Gerar vocabul√°rio usando LLM."""
        try:
            logger.info("ü§ñ Consultando LLM para gera√ß√£o de vocabul√°rio...")
            
            # Gerar usando LangChain (sem cache)
            response = await self.llm.ainvoke(prompt_messages)
            content = response.content
            
            # Tentar parsear JSON
            try:
                # Limpar response se necess√°rio
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].strip()
                
                vocabulary_list = json.loads(content)
                
                if not isinstance(vocabulary_list, list):
                    raise ValueError("Response n√£o √© uma lista")
                
                logger.info(f"‚úÖ LLM retornou {len(vocabulary_list)} itens de vocabul√°rio")
                return vocabulary_list
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"‚ö†Ô∏è Erro ao parsear JSON, tentando extra√ß√£o manual: {str(e)}")
                return self._extract_vocabulary_from_text(content)
                
        except Exception as e:
            logger.error(f"‚ùå Erro na consulta ao LLM: {str(e)}")
            # Usar gera√ß√£o de fallback via IA
            return await self._generate_fallback_vocabulary_ai()
    
    async def _process_and_validate_vocabulary(
        self, 
        raw_vocabulary: List[Dict[str, Any]], 
        enriched_context: Dict[str, Any]
    ) -> List[VocabularyItem]:
        """Processar e validar itens de vocabul√°rio."""
        validated_items = []
        
        unit_ctx = enriched_context["unit_context"]
        
        for i, raw_item in enumerate(raw_vocabulary):
            try:
                # Aplicar valores padr√£o se necess√°rio
                processed_item = {
                    "word": raw_item.get("word", f"word_{i}").lower().strip(),
                    "phoneme": raw_item.get("phoneme", f"/word_{i}/"),
                    "definition": raw_item.get("definition", "defini√ß√£o n√£o dispon√≠vel"),
                    "example": raw_item.get("example", f"Example with {raw_item.get('word', 'word')}."),
                    "word_class": raw_item.get("word_class", "noun"),
                    "frequency_level": raw_item.get("frequency_level", "medium"),
                    "context_relevance": raw_item.get("context_relevance", 0.7),
                    "is_reinforcement": raw_item.get("is_reinforcement", False),
                    "ipa_variant": self._get_ipa_variant(unit_ctx["language_variant"]),
                    "syllable_count": self._estimate_syllable_count(raw_item.get("word", "")),
                }
                
                # Validar usando Pydantic
                vocabulary_item = VocabularyItem(**processed_item)
                validated_items.append(vocabulary_item)
                
            except ValidationError as e:
                logger.warning(f"‚ö†Ô∏è Item {i+1} inv√°lido, pulando: {str(e)}")
                continue
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao processar item {i+1}: {str(e)}")
                continue
        
        logger.info(f"‚úÖ {len(validated_items)} itens validados de {len(raw_vocabulary)} originais")
        return validated_items
    
    async def _apply_rag_filtering(
        self, 
        vocabulary_items: List[VocabularyItem], 
        rag_context: Dict[str, Any]
    ) -> List[VocabularyItem]:
        """Aplicar filtros RAG para evitar repeti√ß√µes e melhorar progress√£o."""
        
        taught_words = set(word.lower() for word in rag_context.get("taught_vocabulary", []))
        reinforcement_candidates = set(word.lower() for word in rag_context.get("reinforcement_candidates", []))
        
        filtered_items = []
        new_words_count = 0
        reinforcement_count = 0
        max_reinforcement = len(vocabulary_items) // 5  # M√°ximo 20% refor√ßo
        
        # Primeiro, adicionar palavras novas
        for item in vocabulary_items:
            word_lower = item.word.lower()
            
            if word_lower not in taught_words:
                # Palavra nova - adicionar
                item.is_reinforcement = False
                filtered_items.append(item)
                new_words_count += 1
                
            elif (word_lower in reinforcement_candidates and 
                  reinforcement_count < max_reinforcement):
                # Palavra para refor√ßo - adicionar com limite
                item.is_reinforcement = True
                filtered_items.append(item)
                reinforcement_count += 1
            
            # Parar se atingiu o n√∫mero desejado
            if len(filtered_items) >= 30:  # Gerar um pouco mais para ter op√ß√µes
                break
        
        logger.info(
            f"üéØ RAG filtering: {new_words_count} novas, {reinforcement_count} refor√ßo"
        )
        
        return filtered_items
    
    # =============================================================================
    # FALLBACKS M√çNIMOS (APENAS PARA ERROS DE IA)
    # =============================================================================
    
    def _minimal_cefr_fallback(self, cefr_level: str) -> str:
        """Fallback m√≠nimo para guidelines CEFR em caso de erro de IA."""
        return f"Vocabul√°rio apropriado para {cefr_level} com foco comunicativo no contexto espec√≠fico"
    
    async def _generate_fallback_vocabulary_ai(self) -> List[Dict[str, Any]]:
        """Gerar vocabul√°rio de fallback usando IA quando LLM principal falha."""
        
        system_prompt = """Voc√™ √© um professor de ingl√™s gerando vocabul√°rio b√°sico de emerg√™ncia.
        
        Gere vocabul√°rio simples e √∫til para estudantes."""
        
        human_prompt = """Gere 5 palavras b√°sicas de vocabul√°rio em ingl√™s no formato JSON:
        
        [
          {
            "word": "palavra",
            "phoneme": "/fonema/",
            "definition": "defini√ß√£o em portugu√™s",
            "example": "Example sentence.",
            "word_class": "noun",
            "frequency_level": "high",
            "context_relevance": 0.8,
            "is_reinforcement": false
          }
        ]"""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Tentar parsear resposta
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            
            vocabulary_list = json.loads(content)
            
            if isinstance(vocabulary_list, list) and len(vocabulary_list) > 0:
                logger.info("‚úÖ Fallback IA gerou vocabul√°rio de emerg√™ncia")
                return vocabulary_list
            else:
                raise ValueError("Fallback IA n√£o retornou lista v√°lida")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Fallback IA tamb√©m falhou: {str(e)}")
            return self._minimal_hardcoded_fallback()
    
    def _minimal_hardcoded_fallback(self) -> List[Dict[str, Any]]:
        """Fallback m√≠nimo com dados hard-coded apenas para emerg√™ncias cr√≠ticas."""
        logger.warning("‚ö†Ô∏è Usando fallback hard-coded m√≠nimo - apenas para emerg√™ncias")
        
        return [
            {
                "word": "vocabulary",
                "phoneme": "/vo äÀàk√¶bj…ôÀål…õri/",
                "definition": "vocabul√°rio",
                "example": "Learning vocabulary is important.",
                "word_class": "noun",
                "frequency_level": "high",
                "context_relevance": 0.7,
                "is_reinforcement": False
            },
            {
                "word": "learning",
                "phoneme": "/Ààl…úrn…™≈ã/",
                "definition": "aprendizagem",
                "example": "Learning English takes practice.",
                "word_class": "noun",
                "frequency_level": "high",
                "context_relevance": 0.7,
                "is_reinforcement": False
            }
        ]
    
    # =============================================================================
    # HELPER METHODS (CONSTANTES T√âCNICAS E UTILIT√ÅRIOS)
    # =============================================================================
    
    def _select_reinforcement_words(self, taught_vocabulary: List[str]) -> List[str]:
        """Selecionar palavras candidatas para refor√ßo."""
        # Algoritmo simples: √∫ltimas 10 palavras ensinadas
        return taught_vocabulary[-10:] if taught_vocabulary else []
    
    def _get_ipa_variant(self, language_variant: str) -> str:
        """Mapear variante de idioma para variante IPA (constante t√©cnica)."""
        return IPA_VARIANT_MAPPING.get(language_variant, "general_american")
    
    def _estimate_syllable_count(self, word: str) -> int:
        """Estimar n√∫mero de s√≠labas (algoritmo t√©cnico simples)."""
        if not word:
            return 1
            
        word = word.lower()
        syllables = 0
        prev_was_vowel = False
        
        for char in word:
            is_vowel = char in VOWEL_SOUNDS
            if is_vowel and not prev_was_vowel:
                syllables += 1
            prev_was_vowel = is_vowel
        
        # Ajustes t√©cnicos
        if word.endswith('e'):
            syllables -= 1
        if syllables == 0:
            syllables = 1
            
        return syllables
    
    def _generate_basic_phoneme(self, word: str) -> str:
        """Gerar fonema b√°sico para palavra (fallback t√©cnico)."""
        # Implementa√ß√£o t√©cnica b√°sica - para casos de emerg√™ncia
        return f"/{word.replace('e', '…ô').replace('a', '√¶')}/"
    
    def _estimate_stress_pattern(self, phoneme: str) -> str:
        """Estimar padr√£o de stress do fonema (an√°lise t√©cnica)."""
        if 'Àà' in phoneme:
            return "primary_first"
        elif 'Àå' in phoneme:
            return "secondary_first"
        else:
            return "unstressed"
    
    def _extract_vocabulary_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extrair vocabul√°rio de texto quando JSON parsing falha (parser t√©cnico)."""
        vocabulary = []
        lines = text.split('\n')
        
        current_item = {}
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Parser t√©cnico para extrair informa√ß√µes b√°sicas
            if 'word:' in line.lower() or '"word"' in line:
                if current_item:
                    vocabulary.append(current_item)
                    current_item = {}
                word = line.split(':')[-1].strip().strip('"').strip(',')
                current_item['word'] = word
            elif 'phoneme:' in line.lower() or '"phoneme"' in line:
                phoneme = line.split(':')[-1].strip().strip('"').strip(',')
                current_item['phoneme'] = phoneme
            elif 'definition:' in line.lower() or '"definition"' in line:
                definition = line.split(':')[-1].strip().strip('"').strip(',')
                current_item['definition'] = definition
            elif 'example:' in line.lower() or '"example"' in line:
                example = line.split(':')[-1].strip().strip('"').strip(',')
                current_item['example'] = example
        
        if current_item:
            vocabulary.append(current_item)
        
        # Preencher campos faltantes com padr√µes t√©cnicos
        for item in vocabulary:
            item.setdefault('word_class', 'noun')
            item.setdefault('frequency_level', 'medium')
            item.setdefault('context_relevance', 0.7)
            item.setdefault('is_reinforcement', False)
        
        return vocabulary
    
    def _parse_phoneme_response(self, response_content: str) -> Dict[str, str]:
        """Parser t√©cnico para extrair fonemas da resposta IA."""
        phoneme_mapping = {}
        lines = response_content.split('\n')
        
        for line in lines:
            if ':' in line and '/' in line:
                parts = line.split(':')
                if len(parts) >= 2:
                    word = parts[0].strip()
                    phoneme_part = parts[1].strip()
                    
                    # Extrair fonema entre /.../ 
                    import re
                    phoneme_match = re.search(r'/[^/]+/', phoneme_part)
                    if phoneme_match:
                        phoneme_mapping[word] = phoneme_match.group()
        
        return phoneme_mapping
    
    # =============================================================================
    # UTILITY METHODS
    # =============================================================================
    
    async def get_service_status(self) -> Dict[str, Any]:
        """Obter status do servi√ßo."""
        return {
            "service": "VocabularyGeneratorService",
            "status": "active",
            "ai_integration": "100% contextual analysis",
            "cache_system": "disabled_as_requested",
            "storage": "supabase_integration",
            "llm_model": self.openai_config.openai_model,
            "constants_maintained": list(IPA_VARIANT_MAPPING.keys()),
            "ai_analysis_methods": [
                "_analyze_cefr_guidelines_ai",
                "_analyze_phonetic_complexity_ai", 
                "_calculate_quality_metrics_ai",
                "_analyze_context_relevance_ai",
                "_enrich_with_phonemes_ai",
                "_improve_phonemes_ai"
            ]
        }
    
    async def validate_generation_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Validar par√¢metros de gera√ß√£o."""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Valida√ß√µes b√°sicas
        required_fields = ["unit_data", "hierarchy_context", "rag_context"]
        for field in required_fields:
            if field not in params:
                validation_result["errors"].append(f"Campo obrigat√≥rio ausente: {field}")
                validation_result["valid"] = False
        
        # Valida√ß√µes espec√≠ficas
        unit_data = params.get("unit_data", {})
        if not unit_data.get("context"):
            validation_result["warnings"].append("Contexto da unidade vazio - pode afetar qualidade")
        
        if not params.get("images_analysis", {}).get("success"):
            validation_result["warnings"].append("An√°lise de imagens n√£o dispon√≠vel")
        
        target_count = params.get("target_vocabulary_count", 25)
        if target_count < 10 or target_count > 50:
            validation_result["warnings"].append(f"Target count {target_count} fora do range recomendado (10-50)")
        
        return validation_result